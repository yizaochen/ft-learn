# ft-learn

## Training Dataset should look like
- https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca?row=0


## Theory
- https://medium.com/@arshren/an-intuitive-guide-to-fine-tuning-large-language-model-a52958c24785

## Reference List
- https://towardsai.net/p/machine-learning/fine-tuning-a-llama-2-7b-model-for-python-code-generation
- https://github.com/edumunozsala/llama-2-7B-4bit-python-coder
- https://predibase.com/blog/fine-tune-a-code-generation-llm-with-llama-2-for-less-than-the-cost-of-a
- https://predibase.com/
- https://github.com/ludwig-ai/ludwig?tab=readme-ov-file